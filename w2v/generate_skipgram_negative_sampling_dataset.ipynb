{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Skip-gram Negative Sampling (SGNS) training dataset for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from w2v_data_utility import *\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_creation_hyperparameters:\n",
    "\n",
    "start_year = 2013\n",
    "end_year = 2023\n",
    "normalisation_level = \"docs_level_sentence_list_text_normalised\"\n",
    "firm_name_replaced_with_abbrevs = True\n",
    "firm_name_replaced_with_abbrevs_global = True\n",
    "min_sentence_len = 10\n",
    "n_gram_range = (1,1)\n",
    "vocab_min_token_freq = 500\n",
    "vocab_max_token_freq = None\n",
    "max_n_top_features = None\n",
    "half_context_size = 2\n",
    "sgns_k = 3\n",
    "\n",
    "# negative_sampling_method = \"selective_uniform\", \"scaled_unselective_unigram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data_and_vocab_config = {\n",
    "    \"start_year\": start_year,\n",
    "    \"end_year\": end_year,\n",
    "    \"normalisation_level\": normalisation_level,\n",
    "    \"firm_name_replaced_with_abbrevs\": firm_name_replaced_with_abbrevs,\n",
    "    \"firm_name_replaced_with_abbrevs_global\": firm_name_replaced_with_abbrevs_global,\n",
    "    \"min_sentence_len\": min_sentence_len,\n",
    "    \"n_gram_range\": n_gram_range,\n",
    "    \"vocab_min_token_freq\": vocab_min_token_freq,\n",
    "    \"vocab_max_token_freq\": vocab_max_token_freq,\n",
    "    \"max_n_top_features\": max_n_top_features,\n",
    "    \"half_context_size\": half_context_size,\n",
    "    \"sgns_k\": sgns_k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parsed PDF: 100%|██████████| 117/117 [00:22<00:00,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# load parsed data\n",
    "\n",
    "master_data_dict = {}\n",
    "time_range = list(\n",
    "    range(\n",
    "        w2v_data_and_vocab_config[\"start_year\"],\n",
    "        w2v_data_and_vocab_config[\"end_year\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "firm_list = os.listdir(Path(\"../../report_data\"))\n",
    "firm_list.sort()\n",
    "for firm in tqdm(firm_list, desc=\"Loading Parsed PDF\"):\n",
    "    master_data_dict[firm] = {}\n",
    "    firm_level_retrieval_path = (\n",
    "        Path(\"../../preprocessed_and_parsed_report_data\") / f\"{firm}_data.json\"\n",
    "    )\n",
    "    with open(firm_level_retrieval_path, \"r\") as file:\n",
    "        firm_dict = json.load(file)\n",
    "\n",
    "    for year in time_range:\n",
    "        master_data_dict[firm][str(year)] = {}\n",
    "        master_data_dict[firm][str(year)][\n",
    "            w2v_data_and_vocab_config['normalisation_level']\n",
    "        ] = firm_dict[firm][str(year)][\n",
    "            w2v_data_and_vocab_config['normalisation_level']\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Firms: 100%|██████████| 117/117 [00:11<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing years: 0\n",
      "Number of usable sentences: 3695273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect usable sentences from parsed data\n",
    "\n",
    "training_data = collect_usable_sentences(\n",
    "    master_data_dict,\n",
    "    time_range, \n",
    "    firm_list,\n",
    "    normalisation_level=w2v_data_and_vocab_config[\"normalisation_level\"],\n",
    "    min_sentence_len=w2v_data_and_vocab_config[\"min_sentence_len\"],\n",
    "    firm_name_replaced_with_abbrevs = w2v_data_and_vocab_config[\"firm_name_replaced_with_abbrevs\"],\n",
    "    firm_name_replaced_with_abbrevs_global = w2v_data_and_vocab_config[\"firm_name_replaced_with_abbrevs_global\"],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n-gram dataset\n",
    "\n",
    "generated_ngram_dataset = generate_ngram_dataset(\n",
    "    training_data, n_range=w2v_data_and_vocab_config[\"n_gram_range\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  76193097\n",
      "Total number of unique tokens:  154836\n"
     ]
    }
   ],
   "source": [
    "# calculate frequency and proportion\n",
    "\n",
    "unique_text_list_with_freq_prop_dict = calculate_frequency_and_proportion(\n",
    "    generated_ngram_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  6045\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "\n",
    "word_to_idx_dict, idx_to_word_dict, vocab_list, vocab_stats_dict = create_vocab(\n",
    "    unique_text_list_with_freq_prop_dict,\n",
    "    min_freq = w2v_data_and_vocab_config['vocab_min_token_freq'],\n",
    "    max_freq = w2v_data_and_vocab_config['vocab_max_token_freq'],\n",
    "    max_n_top_features = w2v_data_and_vocab_config['max_n_top_features'],\n",
    "    create_oov_token = False,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove oov from sentences\n",
    "\n",
    "generated_ngram_dataset_no_oov = remove_oov_from_sentences(\n",
    "    generated_ngram_dataset, word_to_idx_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size:  5\n",
      "Number of context sentence for training:  234178456\n"
     ]
    }
   ],
   "source": [
    "# create skipgram dataset\n",
    "\n",
    "input_list_skipgram, output_list_skipgram , sentence_used_to_create_dataset = create_skipgram_dataset(\n",
    "    sentence_list = generated_ngram_dataset_no_oov,\n",
    "    word_to_idx_dict=word_to_idx_dict,\n",
    "    half_context_size=w2v_data_and_vocab_config['half_context_size'],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_stats_dict = calculate_post_vocab_proportion(\n",
    "    vocab_stats_dict, sentence_used_to_create_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_stats_dict = distribution_scaling(vocab_stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_stats_dict = find_all_positive_negative_output_idx(\n",
    "    output_list_skipgram,\n",
    "    input_list_skipgram,\n",
    "    vocab_stats_dict,\n",
    "    idx_to_word_dict,\n",
    "    word_to_idx_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_output_indices, neg_output_data, input_list_skipgram_array = (\n",
    "    negative_sampling_using_numba_data_prep(\n",
    "        vocab_stats_dict, idx_to_word_dict, input_list_skipgram\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples_list_skipgram = create_skipgram_negative_samples_from_uniform_dist_numba(\n",
    "    neg_output_indices, neg_output_data, input_list_skipgram_array, k=w2v_data_and_vocab_config['sgns_k']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if sampling is done correctly\n",
    "# for ith_idx in np.random.randint(0, len(input_list_skipgram), 10):\n",
    "#     for idx in negative_samples_list_skipgram[ith_idx]:\n",
    "#         print(f'output: {idx_to_word_dict[input_list_skipgram[ith_idx]]}')\n",
    "#         print(f'Sampled negative output: {idx_to_word_dict[input_list_skipgram[idx]]}')\n",
    "\n",
    "#         if idx in vocab_stats_dict[idx_to_word_dict[input_list_skipgram[ith_idx]]][\"pos_output_set\"]:\n",
    "#             print(\"-----SAMPLING DONE INCORRECTLY-----\")\n",
    "#         if idx in vocab_stats_dict[idx_to_word_dict[input_list_skipgram[ith_idx]]][\"neg_output_set\"]:\n",
    "#             print(\"-----PASS-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data_and_vocab_config[\"training_data_stats\"] = {\n",
    "    \"n_usable_sentences\": len(training_data),\n",
    "    \"vocab_size\": len(vocab_list),\n",
    "    \"n_samples\": len(output_list_skipgram),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab related data\n",
    "with open(Path(\"../../w2v_sgns_data/vocab_list.pkl\"), 'wb') as f:\n",
    "    pickle.dump(vocab_list, f)\n",
    "with open(Path(\"../../w2v_sgns_data/vocab_stats_dict.pkl\"), 'wb') as f:\n",
    "    pickle.dump(vocab_stats_dict, f)\n",
    "with open(Path(\"../../w2v_sgns_data/word_to_idx_dict.json\"), 'w') as f:\n",
    "    json.dump(word_to_idx_dict, f, indent=4)\n",
    "with open(Path(\"../../w2v_sgns_data/idx_to_word_dict.json\"), 'w') as f:\n",
    "    json.dump(idx_to_word_dict, f, indent=4)\n",
    "\n",
    "# training data \n",
    "with open(Path(\"../../w2v_sgns_data/input_list_skipgram.pkl\"), 'wb') as f:\n",
    "    pickle.dump(input_list_skipgram, f)\n",
    "with open(Path(\"../../w2v_sgns_data/output_list_skipgram.pkl\"), 'wb') as f:\n",
    "    pickle.dump(output_list_skipgram, f)\n",
    "with open(Path(\"../../w2v_sgns_data/negative_samples_list_skipgram.pkl\"), 'wb') as f:\n",
    "    pickle.dump(negative_samples_list_skipgram, f)\n",
    "\n",
    "# metadata of data vocab and training data\n",
    "with open(Path(\"../../w2v_sgns_data/w2v_data_and_vocab_config.json\"), 'w') as f:\n",
    "    json.dump(w2v_data_and_vocab_config, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.11.8_venv_kernel",
   "language": "python",
   "name": "py_3.11.8_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
